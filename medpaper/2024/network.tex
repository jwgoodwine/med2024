\section{NEURAL NETWORK, TRAINING DATA SET, AND TRAINING}
\label{sec:network}

  The neural network used in this paper is illustrated in
  Figure~\ref{fig:thennet}. There is an input layer with 101 nodes, a hidden
  layer with 64 nodes, a subsequent hidden layer with 16 nodes, another hidden
  layer with 16 nodes, and an output layer with 1 node. Each hidden layer has
  the ReLU activation function. The values input to the network is the unit step
  response of a linear system in the time range of $0 \leq t \leq 10$
  discretized into time steps of $\Delta t = 0.1$ [s], which gives 101 input
  nodes. The single output of the network is the order of the system that
  produced the input step response. 

The network was implemented in python using the \texttt{torch} library and
  \texttt{pytorch\_lightning} tools. This network is not trained as a classifier
  because we want it to be able to generalize first and second order systems to
  fractional orders between them, so the loss function is the mean squared error
  function, \texttt{mse\_loss()}, and the optimization method adopted was Adam
  optimizer, \texttt{torch.optim.Adam()} with a learning rate of $0.001$. A
  branch of our github repository that should repeatably replicate the results
  presented in this paper is at \cite{Goodwine_Integer_trained_neural}.

\subsection{Integer Order Training, Validation and Testing Data}

  An individual element of the training, validation and training sets is the
  step response for a first or second order system (not fractional order). The
  manner in which they are generated is:

  \begin{itemize}

    \item Select a value from a uniform random distribution between 0 and 1, and
    if the value is less than 0.5, then the step response will be for a second
    order system, and if not, then it will be for a first order system.

    \item Select two numbers, $c_1$ and $c_2$ from a uniform distribution with
    values between 0.01 and 4.

    \begin{itemize}

      \item If the response is for a first order system, then the transfer
      function is
      \[
        G(s) = \frac{c_2}{c_1 s + c_2}.
      \]

      \item If the response is for a second order system, then the selected
      transfer function is
      \[
        G(s) = \frac{c_2}{c_1 s^2 + c_2}.
      \]

    \end{itemize}

	\item The unit step response from 0 to 10 seconds for the transfer function
	  is generated using the \texttt{control.step\_response()} function from the
	  python control system library. The number of time steps in the response is
	  determined by the step response function. It is then sampled every 0.1
	  second so that the length of the response vector is 101.

\end{itemize} 

    Using this method we generate a set of 100,000 first or second order step
    responses with approximately the same number of first and second order
    responses. The training set is split into three subsets: 60,000 training
    elements, 20,000 validation elements and 20,000 testing elements. For
    training, the training set is shuffled at the beginning of each epoch and
    the optimization method is applied to change the weights in the network.  At
    the end of the epoch, the network is run on the validation set to compute an
    error for data points the network was not trained on. Evidence of
    over training would be if the validation set error decreases and then
    increases.  At the end of all the training, the error for the testing set is
    computed.

 
Figure~\ref{fig:epochs} illustrates the error on the validation set for 10
training runs for the network versus epoch. It appears that if the validation
error increases, it tends to start to do so around 1000 epochs; otherwise it
tends to stop changing around 1000 epochs (there seems to be one exception). As
such, we will fix the number of training epochs at 1000.  Additionally, various
alternative configurations to the network described above, including more hidden
nodes and different activation functions were investigated, and the
configuration presented above generally gave the best results. 

\begin{figure}
\centering
\input{overtrain.pgf}
\vspace*{-5pt}
\caption{Neural network output error on validation (not training) set versus
training epoch.}
\label{fig:epochs}
\end{figure}

