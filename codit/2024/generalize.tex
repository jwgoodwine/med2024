\section{USING THE INTEGER TRAINED NETWORK ON FRACTIONAL ORDER STEP RESPONSES}
\label{sec:generalize}

Now we apply a fractional order step response to the input layer of the trained
neural network to see if it can generalize the training on first and second
order transfer functions to fractional order step responses. As indicated above,
we use the \texttt{numfracpy} python library to numerically compute the
fractional order step response for time from 0 to 10. The solution is
numerically computed with a time step of $\Delta t = 0.01$, but the input to the
network is 101 nodes, so only every 10th element of the numerical solution is
used.

\subsection{Generating the test set}
We tested the network on 1000 fractional order step responses to transfer
functions of the form
\[
  X(s) = \left( \frac{k}{s^\alpha + k} \right) \left( \frac{1}{s} \right),
\]
where $\alpha$ is the fractional order of the derivative, or in the time domain
\begin{equation}
\frac{\d^\alpha x}{\d t^\alpha}(t) + k x(t) = k
\label{eq:fracstep}
\end{equation}
with zero initial conditions. 

The 1000 responses were each generated and tested as follows: 
\begin{itemize}
\item Randomly select an order between 1 and 2 from a uniform distribution.
\item Randomly select a $k$ between 5 and 9 from a uniform distribution. This
  range of values was selected to produce responses with a period of oscillation
  similar to the natural frequencies in the training set.
\item Numerically compute the step response to Equation~\ref{eq:fracstep} using
the \texttt{FODE()} function from the \texttt{numfracpy} library. 
\end{itemize}

\begin{figure}
\centering
\input{fracsteps.pgf}
\vspace*{-5pt}
\caption{First 100 fractional order step responses generated and used for the
neural network.}
\label{fig:steps}
\end{figure}

\subsection{Neural network identifying the order of the test set}

Each of the 1000 fractional order step responses was interpolated or sampled
into time steps of $\Delta t = 0.1$, and those points were applied to the input
layer of the neural network trained on the integer order data.
Figure~\ref{fig:accuracy} is the main result in this paper.  The results that
are illustrated in that figure illustrate an excellent match. Each blue dot in
the plot corresponds to one element of the fractional test set.  Perfect
matching would result in the data forming a straight line from the point $(1,1)$
to $(2,2)$. The mean square error for the data is 0.00121 with $R^2 = 0.9897$.

\begin{figure}
\centering
\input{predicted.pgf}
\vspace*{-5pt}
\caption{Comparison of actual fractional orders and predicted orders.}
\label{fig:accuracy}
\end{figure}

Again, we emphasize that the neural network was only trained at 1 and 2, so all
of the points along the diagonal line between those values can reasonably be
interpreted as a result of the neural network generalizing the definition of the
derivative. The largest deviation from perfect generalization is near the two
endpoints. A speculative, but plausable, explanation of those is that because
those points are near the training points, the network will tend to over predict
orders of 1 and 2.

In designing the neural network, we tested the efficacy of the network on a wide
range of configurations; specifically, with the number of nodes in the first
hidden layer ranging from 32 to 128 and the number of nodes in the second hidden layer
ranging from 16 to 64. We also tested ReLU and sigmoid activation functions. As
a measure of efficacy in predicting the fractional order, the $R^2$ ranged from
0.989 for the network configuration presented to 0.95 for sigmoid activation
functions with the same configuration of hidden nodes to 0.92 with a higher
number of hidden nodes. 
